\chapter{Statistics}

\section{Brute force}
\subsection{Statistical relations}


Confidence interval at $1-\alpha$ level:
\begin{equation}\label{interval}
\bar{x} \pm z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{var(x)}{N}}
\end{equation}

Based on \autoref{interval} assuming a interval threshold, A
\begin{equation}\label{interval2}
N \geq \left(z_{\frac{\alpha}{2}} \cdot \frac{\sqrt{var(x)}}{A} \right)^2
\end{equation}

Using the normal approximation of the binomial proportion we  estimate the number of samples with 90\% confidence level and an interval threshold of $\pm 25\%$ or 1 dB.Based on the Equation 1.12 we have for $ \bar{x} = 10^{-5} $ :
\begin{equation}\label{sampleEQ}
N=(1.645)^{2} \cdot \frac{1}{(0.25)^{2}} \cdot \frac{1-\bar{x}}{\bar{x}} = 4.3 \cdot 10^{6}
\end{equation}
Because the above procedure is an approximation we would generate $ 10 \cdot 10^{6} $ number of samples.\citep{SampleNumURC}
\subsection{Limitation for measurement purposes}

Strict limitations
\begin{equation}
SNRs > 1
\end{equation}

Other limitations
\begin{equation}
E\left(SNR\right) \;>\, \frac{1}{raylinv(p,\theta)} 
\end{equation}

\begin{where}
\va{$x$}{is the individual sample}{NA}
\va{$xs$}{is a threshold value for the samples}{NA}
\va{$\theta$}{is the mode of the distribution}{NA}
\va{$SNRs$}{is the threshold value of the signal-to-noise ratio}{1}
\va{$SNR$}{is the individual sample of the signal-to-noise ratio}{1}
\va{$N$}{number of samples}{1}
\va{$z_{\frac{\alpha}{2}}$}{is the normalize interval from a standard distribution}{NA}
\end{where}



\section{Statistics method for rare events}
As seen in previous chapters, there is a need of a high number of samples, to get a high confidence interval. The reason for this, is there is a need of a high number of rare events to occur to get a high confidence interval for this events. 

For simulations there have been developed some methods to handle this problem. One of them is importance sampling. Importance sampling changes the distribution where the samples is taken from. By changing the distribution, so the rare events is not rare anymore and a high number of them can be samples with a smaller sample population. When introduction this new distribution, there also needs to be introduced a weighting factor. This weighting factor is defined from the change in the different distribution. This weighting factor is used to go from the actually distribution to the new created distribution.

Most of the other methods used in simulation to handle this problem with rare events, changes the distribution, as this is one of the known parameters for simulations. A problem when using these methods, is that in the real world the exactly distribution is not known, so weighting factor, that is the difference between the real and new created distribution can only be a estimate. But these methods can not be used in the real world to reduce the number of samples needed needed for this measurement campaign, as the distribution in the real world would have to be force fulled changed, so the rare events will happen more often. To do that, more deep fade spots have to be introduced into the room used for measurements, which can be done. But the measurements will not be the true picture of the distribution, unless the weighting factor is calculated precise enough and the new distribution is known.



%Reduction of number of samples
%- Important sampling
%- Not useful

\section{Bootstrap}
Another statistically aspect, which there are different method and techniques for, is how to determent the confidence interval at a certain confidence level. From the previous calculation of the number of samples needed in section \todo{Missing ref to section}, there is used a confidence level of 90\% \todo{Check this}. This is attain from doing a high number of measurements. This high number of measurements will be very time consuming. Hence there can be look at on a statistically method to determent the confidence interval, by having a lower number of samples. One of these methods is bootstrap.

Bootstrap is a method, where one take a large amount of samples from big sample space. This surrogate population, as the measurements made up, will then be a estimate of true sample population. From this surrogate population a series of phantom samples are made. This is done with making a simulation, where there are used the same number of samples, as in the original measurements, where the samples are picked uniformly from the surrogate population. This process is done a few thousand times, where the wanted value wanted to measured can be put into a histogram, where normale usage of getting the confidence level can be used. As in case of a interval of 90\%, the upper and lower 5\% will be cut off and this will give the confidence interval.

This method will still need a high number of samples, as there is still needed for some rare events to occur. But the sample size do not need to be as large as seen in section \todo{More refs}. This comes from that there will instead be used computation power instead to generate these phantom samples, which replaces the many other real life sample. 

\todo{Need example. A example used an example of our project, but on $10^{-3}$ would be great i think}






%\subsection{Statistics modeling from measurements}
%Model/Regression (Maximum likely hood)
%- Usage of bootstraping to estimate the a's in regression